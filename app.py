from pathlib import Path
import streamlit as st
from streamlit_chat import message
from huggingface_hub import InferenceClient
from langchain import HuggingFaceHub
import requests# Internal usage
import os
from dotenv import load_dotenv
from time import sleep
import uuid
import sys
from streamlit_extras.colored_header import colored_header
from streamlit_extras.add_vertical_space import add_vertical_space

st.set_page_config(page_title="AI Chatbot 100% Free", layout="wide")
st.write('å®Œå…¨å¼€æºå…è´¹çš„AIæ™ºèƒ½èŠå¤©åŠ©æ‰‹ | Absolute Free & Opensouce AI Chatbot')

css_file = "main.css"
with open(css_file) as f:
    st.markdown("<style>{}</style>".format(f.read()), unsafe_allow_html=True)

load_dotenv()
yourHFtoken = os.getenv("HUGGINGFACEHUB_API_TOKEN")
repo_id=os.getenv("repo_id")

#AVATARS
#av_us = './man.png' #"ğŸ¦–" #A single emoji, e.g. "ğŸ§‘ ğŸ’»", "ğŸ¤–", "ğŸ¦–". Shortco
#av_ass = './robot.png'
av_us = 'ğŸ§‘'
av_ass = 'ğŸ¤–'
# Set a default model
#if "hf_model" not in st.session_state:
#    st.session_state["hf_model"] = "HuggingFaceH4/starchat-beta"

def starchat(model,myprompt, your_template):
    from langchain import PromptTemplate, LLMChain
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = yourHFtoken
    llm = HuggingFaceHub(repo_id=model,
                         model_kwargs={"min_length":100,
                                       "max_new_tokens":1024, "do_sample":True,
                                       "temperature":0.1,
                                       "top_k":50,
                                       "top_p":0.95, "eos_token_id":49155})
    template = your_template
    prompt = PromptTemplate(template=template, input_variables=["myprompt"])
    llm_chain = LLMChain(prompt=prompt, llm=llm)
    add_notes_1="Beginning of chat history:\n"
    add_notes_2="End of chat history.\n"
    add_notes_3="Please consult the above chat history before responding to the user question below.\n"
    add_notes_4="User question: "
    myprompt_temp=myprompt
    myprompt = add_notes_1 + "\n" + contexts + "\n" + add_notes_2 + "\n" + add_notes_3 + "\n"+ add_notes_4 + "\n" + myprompt
    st.write("---åœ¨def starchat(model,myprompt, your_template)å†…çš„ä¿¡æ¯æ‰“å°è¾“å‡ºå¼€å§‹")
    st.write("Current User Query: "+myprompt_temp)    
    st.write("Combined User Input as Prompt:")
    st.write(myprompt)
    st.write("---åœ¨def starchat(model,myprompt, your_template)å†…çš„ä¿¡æ¯æ‰“å°è¾“å‡ºç»“æŸ")
    llm_reply = llm_chain.run(myprompt)    
    reply = llm_reply.partition('<|end|>')[0]
    return reply

if "file_name" not in st.session_state:
    st.session_state["file_name"] = str(uuid.uuid4()) + ".txt"
    st.write("éšæœºç”Ÿæˆçš„æ–‡ä»¶åç§°ï¼š"+st.session_state["file_name"])

def writehistory(text):       
    st.write("éšæœºç”Ÿæˆçš„æ–‡ä»¶åç§°ï¼š"+st.session_state["file_name"])
    with open(st.session_state["file_name"], 'a+') as f:
        f.write(text)
        f.write('\n')
        f.seek(0) 
        contexts = f.read()
        st.write("contextsçš„å†…å®¹ï¼š"+contexts)
    return contexts

if "messages" not in st.session_state:
   st.session_state.messages = []
for message in st.session_state.messages:
   if message["role"] == "user":
#      with st.chat_message(message["role"],avatar=av_us):
      with st.chat_message(message["role"]):
           st.write("è¿™é‡Œæ˜¯ç”¨æˆ·è¾“å…¥çš„å†å²ä¿¡æ¯æ˜¾ç¤º")           
           st.markdown(message["content"])           
   else:
#       with st.chat_message(message["role"],avatar=av_ass):
       with st.chat_message(message["role"]):
           st.write("è¿™é‡Œæ˜¯assistantå›å¤çš„å†å²ä¿¡æ¯æ˜¾ç¤º")           
           st.markdown(message["content"])           

if myprompt := st.chat_input("Enter your question here."):    
    st.session_state.messages.append({"role": "user", "content": myprompt})    
#    with st.chat_message("user", avatar=av_us):
    with st.chat_message("user"):
        st.write("---ç”¨æˆ·çš„å½“å‰è¾“å…¥é—®é¢˜æ˜¾ç¤ºå¼€å§‹---")
        st.markdown(myprompt)
        st.write("---ç”¨æˆ·çš„å½“å‰è¾“å…¥é—®é¢˜æ˜¾ç¤ºç»“æŸ---")
        usertext = f"user: {myprompt}"      
        contexts = writehistory(usertext)  
        st.write("åœ¨ç”¨æˆ·å½“å‰è¾“å…¥é—®é¢˜çš„æ¨¡å—è°ƒç”¨writehistoryå†™å…¥èŠå¤©å†å²è®°å½•çš„å‡½æ•°/æ–¹æ³•ï¼Œä¼šæ‰“å°è¾“å‡ºæ–‡ä»¶åç§°ï¼Œå¹¶è¾“å‡ºæ­¤æ—¶çš„user-contextså†…å®¹")        
    with st.chat_message("assistant"):
        with st.spinner("AI Thinking..."):            
            st.markdown("st.markdownæ–¹æ³•æ˜¾ç¤ºï¼šassistantçš„æœ¬æ¬¡/å½“å‰å›å¤ç»“æœæ˜¾ç¤ºä½ç½®ä»è¿™é‡Œå¼€å§‹ - è¾“å‡ºå¼€å§‹...")
            message_placeholder = st.empty() 
            full_response = ""
            st.write("å¼€å§‹è°ƒç”¨starchatå‡½æ•°")
            res = starchat(
#                  st.session_state["hf_model"],
                  repo_id,
                  myprompt, "<|system|>\n<|end|>\n<|user|>\n{myprompt}<|end|>\n<|assistant|>")
            st.write("starchatå‡½æ•°è°ƒç”¨ç»“æŸ")
            response = res.split(" ")            
            for r in response:
                full_response = full_response + r + " "
                message_placeholder.markdown(full_response + "|")
                sleep(0.1)                        
            st.markdown("st.markdownæ–¹æ³•æ˜¾ç¤ºï¼šassistantçš„æœ¬æ¬¡/å½“å‰å›å¤ç»“æœæ˜¾ç¤ºä½ç½®åˆ°è¿™é‡Œç»“æŸ - è¾“å‡ºç»“æŸ...")            
            st.write("å¼€å§‹æ˜¾ç¤ºå®Œæ•´çš„AI Response")
            #message_placeholder.markdown(full_response)
            st.write("å®Œæ•´çš„AI Responseæ˜¾ç¤ºç»“æŸ")
            asstext = f"assistant: {full_response}" 
            contexts = writehistory(asstext)
            st.write("åœ¨assistantå½“å‰å›å¤çš„æ¨¡å—è°ƒç”¨writehistoryå†™å…¥èŠå¤©å†å²è®°å½•çš„å‡½æ•°/æ–¹æ³•ï¼Œä¹Ÿä¼šæ‰“å°è¾“å‡ºæ–‡ä»¶åç§°ï¼Œå¹¶è¾“å‡ºæ­¤æ—¶çš„assitant-contextså†…å®¹")            
            st.write("st.chat_messageçš„assistantä¹‹contextsï¼ˆè¿™é‡Œä¼šå°†å½“å‰/æœ¬æ¬¡çš„AIå›å¤å†…å®¹è¿½åŠ åˆ°contextsæœ«å°¾ï¼‰: "+contexts)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
